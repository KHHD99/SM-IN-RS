{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MICROSOFT RECOMMENDER, SAR ALGORITHM  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy, logging, sys, warnings, joblib\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from recommenders.models.sar import SAR \n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.utils.python_utils import binarize\n",
    "from recommenders.datasets.python_splitters import python_stratified_split\n",
    "from recommenders.evaluation.python_evaluation import (precision_at_k, mae, rsquared)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a list of top_k items that will be recommended to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_top_k=[1,3,5,7,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MOVIELENS_DATA_SIZE = \"100k\"\n",
    "data = movielens.load_pandas_df(size=MOVIELENS_DATA_SIZE)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # local loading \n",
    "# data=pd.read_csv('ML_100K.csv', sep='\\t' )\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the ratings to float32 to reduce memory usage\n",
    "data['rating'] = data['rating'].astype(np.float32)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-folds Cross-validation\n",
    " Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 4\n",
    "kf = KFold(n_splits=n_folds, shuffle=False)\n",
    "folds= kf.split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the list of the similarities that will be investigated\n",
    "similarity type must be one  of available similarity metrics:\n",
    "\t\n",
    "    \"SOKAL_MICHENER\", \"SOKAL_SNEATH_II\", \"SOKAL_SNEATH_IV\", \"SOKAL_SNEATH_V\",  \"PEARSON_I\", \"PEARSON_II\", \"PEARSON_III\", \"PEARSON_HERON_I\", \"PEARSON_HERON_II\", \"BARONI_URBANI_BUSER_I\", \"BARONI_URBANI_BUSER_II\", \"FORBES_I\", \"FORBES_II\", \"YULEQ\", \"YULEQ_W\", \"TARANTULA\", \"AMPLE\", \"ROGERS_TANIMOTO\", \"FAITH\", \"GOWER_LEGENDRE\", \"INNERPRODUCT\", \"RUSSELL_RAO\", \"TARWID\", \"DENNIS\", \"GOWER\", \"STILES\", \"FOSSUM\", \"DISPERSON\", \"HAMANN\", \"MICHAEL\", \"PEIRCE\", \"EYRAUD\", \"YULEQ_D\", \"MEAN_MANHATTAN\", \"VARI\", \"SHAPEDIFFERENCE\", \"PATTERNDIFFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the list of the similarity metrics with negative co-occurrences\n",
    "list_metrics_d=[\n",
    "# similarities with negative co-occurrences \n",
    "\"SOKAL_MICHENER\", \"SOKAL_SNEATH_II\", \"SOKAL_SNEATH_IV\", \"SOKAL_SNEATH_V\",  \"PEARSON_I\", \n",
    "\"PEARSON_II\", \"PEARSON_III\", \"PEARSON_HERON_I\", \"PEARSON_HERON_II\", \"BARONI_URBANI_BUSER_I\", \n",
    "\"BARONI_URBANI_BUSER_II\",  \"FORBES_I\",  \"FORBES_II\", \"YULEQ\", \"YULEQ_W\", \"TARANTULA\",  \"AMPLE\",\n",
    "\"ROGERS_TANIMOTO\", \"FAITH\",  \"GOWER_LEGENDRE\", \"INNERPRODUCT\", \"RUSSELL_RAO\", \"TARWID\",\n",
    "\"DENNIS\", \"GOWER\",  \"STILES\", \"FOSSUM\", \"DISPERSON\",  \"HAMANN\",  \"MICHAEL\", \"PEIRCE\", \"EYRAUD\",\n",
    "\n",
    "# distances with negative co-occurrences \n",
    "\"YULEQ_D\", \"MEAN_MANHATTAN\", \"VARI\", \"SHAPEDIFFERENCE\", \"PATTERNDIFFERENCE\" \n",
    "]\n",
    "\n",
    "print(len(list_metrics_d),'similarity metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization  of models\n",
    "    For each metric in list_metrics_d, a new SAR model is initialized with specific parameters:\n",
    "\n",
    "- col_user, col_item, col_rating, and col_timestamp specify the column names in the dataset.\n",
    "- similarity_type is set to the current metric from the loop.\n",
    "- time_decay_coefficient is set to 30.\n",
    "- normalize is set to True to normalize similarity scores.\n",
    "- timedecay_formula is set to True to apply the time decay form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models=[]\n",
    "for metric in list_metrics_d:\n",
    "    model                  = SAR(\n",
    "    col_user               = \"userID\",\n",
    "    col_item               = \"itemID\",\n",
    "    col_rating             = \"rating\",\n",
    "    col_timestamp          = \"timestamp\",\n",
    "\n",
    "    similarity_type        =  metric,   \n",
    "    time_decay_coefficient =  30, \n",
    "    normalize              =  True, \n",
    "    timedecay_formula      =  True\n",
    "    )\n",
    "    list_models.append(model)\n",
    "print('Initiated models : ',len(list_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory for saving models\n",
    "base_dir = 'Models_ML_100K_cv/'\n",
    "\n",
    "# Initialize lists to store evaluation metrics across models and folds\n",
    "list_list_PRECISION, list_list_MAE, list_list_R_SQUARED = [], [], []\n",
    "\n",
    "# Start a timer to measure the training and evaluation time\n",
    "with Timer() as train_time:\n",
    "    \n",
    "    # Iterate over each model and its corresponding metric\n",
    "    for model_index, model in enumerate(list_models):\n",
    "        \n",
    "        # Get the metric name corresponding to the current model\n",
    "        metric = list_metrics_d[model_index]\n",
    "        print(f\"Starting evaluation for model {metric}\")\n",
    "        \n",
    "        # Initialize lists to store metrics for each fold\n",
    "        fold_list_MAE, fold_list_PRECISION, fold_list_R_SQUARED = [], [], []\n",
    "\n",
    "        # Perform k-fold cross-validation\n",
    "        for fold_index, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "            # List to store top-k recommendations for each fold\n",
    "            model_K_items=[] \n",
    "            \n",
    "            # Split data into training and test sets\n",
    "            train, test = data.iloc[train_index], data.iloc[test_index]  \n",
    "           \n",
    "            # Fit the model on the training data of the current fold\n",
    "            model.fit(train)\n",
    "            \n",
    "            # Save the model to disk to make checkpoints\n",
    "            print(f\"Model {model_index+1} ({metric}) on Fold {fold_index+1} trained.\")\n",
    "            filename = f'model_{model_index+1}_{metric}_fold_{fold_index+1}.sav'\n",
    "            full_path = f'{base_dir}{filename}'\n",
    "            joblib.dump(model, full_path)\n",
    "            print(f\"Model {model_index+1} ({metric}) on Fold {fold_index+1} saved.\")\n",
    "\n",
    "\n",
    "            # Load the model from disk\n",
    "            filename = f'model_{model_index+1}_{metric}_fold_{fold_index+1}.sav'\n",
    "            full_path = f'{base_dir}{filename}'\n",
    "            model = joblib.load(full_path)\n",
    "            print(f\"Model {metric} has been loaded\")\n",
    "\n",
    "            # Top k recommendation  \n",
    "            for i in list_top_k:\n",
    "                print(f'Recommending Top_{i} for fold {fold_index+1}')\n",
    "                # Generate top-k recommendations for the current fold\n",
    "                model_K_items.append(model.recommend_k_items(test,i,remove_seen=True))\n",
    "                print(f'Top_k_{i} is done')\n",
    "            \n",
    "            # Evaluation\n",
    "            ## Initialize lists to store individual metric values for each top-k\n",
    "            list_MAE_1,list_PRECISION_1,list_R_SQUARED_1=[], [], []\n",
    "            j=1\n",
    "            \n",
    "            # Evaluate the recommendations\n",
    "            for index, top_k in enumerate(model_K_items):\n",
    "                # Calculate MAE for the current top-k recommendations\n",
    "                list_MAE_1.append(mae(test, top_k, col_user='userID', col_item='itemID', col_rating='rating'))\n",
    "                # Calculate R-Squared for the current top-k recommendations\n",
    "                list_R_SQUARED_1.append(rsquared(test, top_k, col_user='userID', col_item='itemID', col_rating='rating'))\n",
    "                # Calculate Precision at k for the current top-k recommendations\n",
    "                list_PRECISION_1.append(precision_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=list_top_k[index]))\n",
    "                j+=1\n",
    "           \n",
    "            # Append the evaluation metrics for the current fold\n",
    "            fold_list_MAE.append(list_MAE_1)\n",
    "            fold_list_PRECISION.append(list_PRECISION_1)\n",
    "            fold_list_R_SQUARED.append(list_R_SQUARED_1)\n",
    "        \n",
    "        # Calculate the mean of the evaluation metrics across all folds\n",
    "        mean_MAE = np.mean(fold_list_MAE, axis=0)\n",
    "        mean_PRECISION = np.mean(fold_list_PRECISION, axis=0)\n",
    "        mean_R_SQUARED = np.mean(fold_list_R_SQUARED, axis=0)\n",
    "\n",
    "        # Append the mean metrics for the current model to the respective lists\n",
    "        list_list_MAE.append(mean_MAE)\n",
    "        list_list_PRECISION.append(mean_PRECISION)\n",
    "        list_list_R_SQUARED.append(mean_R_SQUARED)\n",
    "        print(f\"Model {metric} is done\")\n",
    "                \n",
    "# Print the total time taken for training and evaluation       \n",
    "print(f\"Took {train_time.interval} seconds for training and evaluating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for binarizing ratings\n",
    "positivity_threshold = 2\n",
    "\n",
    "# Create a copy of the test dataset to avoid modifying the original data\n",
    "test_bin = test.copy()\n",
    "\n",
    "# Binarize the ratings in the copied test dataset\n",
    "# Convert ratings into binary values based on the positivity_threshold\n",
    "# Ratings >= positivity_threshold are set to 1 (positive), Ratings < positivity_threshold are set to 0 (negative)\n",
    "test_bin['rating'] = binarize(test_bin['rating'], positivity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory for saving results\n",
    "output_directory = 'ResultsTables_ML_100K_cv/'\n",
    "# Iterate over each value in the list of top-k values\n",
    "for i in range(len(list_top_k)):\n",
    "    # Create a dictionary to store the evaluation metrics for the current top-k value\n",
    "    data = {\n",
    "        # \"Metric\": list_metrics_d,\n",
    "        \"Top K\": [list_top_k[i]] * len(list_metrics_d),\n",
    "        \"MAE\": [list_list_MAE[j][i] for j in range(len(list_list_MAE))],\n",
    "        \"Precision@K\": [list_list_PRECISION[j][i] for j in range(len(list_list_PRECISION))],\n",
    "        \"R2\": [list_list_R_SQUARED[j][i] for j in range(len(list_list_R_SQUARED))]\n",
    "    }\n",
    "    # Create a dataframe from the dictionary\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    dataframe.index=list_metrics_d\n",
    "    # Define the output file path for saving the dataframe as an Excel file\n",
    "    output_file = os.path.join(output_directory, f\"Evaluation_Matrix_{list_top_k[i]}.xlsx\")\n",
    "    # Save the dataframe to an Excel file\n",
    "    dataframe.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe\n",
    "output_directory = 'ResultsPlots_ML_100K_cv/'\n",
    "# Iterate over the list of top-k values\n",
    "for i in range(len(list_top_k)):\n",
    "    # Create a figure with 3 subplots arranged in 1 row and 3 columns, with a specified figure size\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 8))\n",
    "\n",
    "    # Sort the data by the 'MAE' column in ascending order\n",
    "    data = data.sort_values(by=['MAE'], ascending=True)\n",
    "    # Plot the 'MAE' values as a bar chart on the first subplot\n",
    "    data['MAE'].plot(ax=axes[0], kind='bar', color='r', y='MAE', x='Similarity', legend='MAE')\n",
    "\n",
    "    \n",
    "    # Sort the data by the 'Precision@K' column in descending order\n",
    "    data = data.sort_values(by=['Precision@K'], ascending=False)\n",
    "    # Plot the 'Precision@K' values as a bar chart on the second subplot\n",
    "    data['Precision@K'].plot(ax=axes[1], kind='bar', color='g', y='Precision@K', x='Similarity', legend='Precision@K')\n",
    "    \n",
    "    # Sort the data by the 'R2' column in descending order\n",
    "    data = data.sort_values(by=['R2'], ascending=False)\n",
    "    # Plot the 'R2' values as a bar chart on the third subplot\n",
    "    data['R2'].plot(ax=axes[2], kind='bar', color='b', y='R2', x='Similarity', legend='R2')\n",
    "    \n",
    "    # Set the title of the entire figure to indicate the current top-k value being evaluated\n",
    "    plt.suptitle(f'Top K = {list_top_k[i]}',fontsize=15)\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "    \n",
    "    # Save the figures\n",
    "    output_file = os.path.join(output_directory, f\"100K_MAE_Precision@K_R²_Top_{list_top_k[i]}_D.jpg\")\n",
    "    fig.savefig(output_file, bbox_inches='tight', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tThe end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
